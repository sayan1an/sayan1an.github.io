1
00:00:00,040 --> 00:00:04,920
Greetings everyone, my name is Sayantan. 
I am excited present our paper Efficient

2
00:00:04,920 --> 00:00:08,600
Graphics Representation with 
Differentiable Indirection.

3
00:00:08,600 --> 00:00:13,080
This is a work in collaboration with Carl 
Marshall, Derek Nowrouzezahrai, Zhao Dong

4
00:00:13,080 --> 00:00:19,520
and Zhengqin Li.
Let us start with

5
00:00:19,520 --> 00:00:23,600
what differentiable indirection is.
So differentiable indirection is very

6
00:00:23,600 --> 00:00:28,680
much like pointer indirection, where we query 
a location using an index or an address

7
00:00:28,680 --> 00:00:33,840
which stores the address to the next location
which contains the final output.

8
00:00:33,840 --> 00:00:40,600
However, in our case we learn these 
pointer values using gradient descent.

9
00:00:40,600 --> 00:00:45,280
Next, we look at why one should be excited 
to use differentiable indirection.

10
00:00:45,280 --> 00:00:50,160
The first reason is efficiency.
Compared to Multi-layered perceptrons

11
00:00:50,160 --> 00:00:53,600
our technique is order of magnitude 
more compute efficient,

12
00:00:53,600 --> 00:00:59,240
and compared to feature grid representations, we 
are order of magnitude more space efficient.

13
00:00:59,240 --> 00:01:03,800
Another reason to use to differentiable 
indirection is its flexibility. We may use the

14
00:01:03,800 --> 00:01:09,080
technique in a variety applications such as
Signed Distance Field representations,

15
00:01:09,080 --> 00:01:12,200
texture compression,
Texture filtering,

16
00:01:12,200 --> 00:01:17,440
Complex BRDFs,
and for compressing radiance fields.

17
00:01:17,440 --> 00:01:21,840
Not only that, but our technique is also very 
generic and potentially extends to applications

18
00:01:21,840 --> 00:01:24,600
beyond graphics.
Finally,

19
00:01:24,600 --> 00:01:29,840
our technique exhibits properties of adaptive
spatial resolution using a single indirection.

20
00:01:31,760 --> 00:01:35,480
In contrast, tree based representations 
such as an oct-tree or a kd-tree

21
00:01:35,480 --> 00:01:40,920
require several levels of indirections 
to produce good quality results.

22
00:01:40,920 --> 00:01:44,560
This translates to our representation 
being more memory coherent compared

23
00:01:44,560 --> 00:01:51,040
to tree-based representations.
So to summarize, our technique is adaptive,

24
00:01:51,040 --> 00:01:58,800
efficient and flexible, with wide variety 
of applications in graphics and beyond.

25
00:01:58,800 --> 00:02:02,480
Next, we look at the technical details 
of differentiable indirection.

26
00:02:02,480 --> 00:02:07,200
At minimum, our technique requires two arrays.
We call the first array storing the

27
00:02:07,200 --> 00:02:10,160
pointers as primary,
and the second array,

28
00:02:10,160 --> 00:02:15,520
storing the output values, as cascaded.
In the forward pass, we first query the primary

29
00:02:15,520 --> 00:02:19,320
array with an input coordinate "x",
and fetch the cells bounding the

30
00:02:19,320 --> 00:02:22,600
query location.
We pass the cell values

31
00:02:22,600 --> 00:02:28,040
through a constraining non-linearity F.
The purpose of the non-linearity is to bound

32
00:02:28,040 --> 00:02:32,520
the output values between 0 to 1 such that 
it points to a valid memory location in the

33
00:02:32,520 --> 00:02:35,600
cascaded array.
At this juncture,

34
00:02:35,600 --> 00:02:39,280
we note the constraining non-linearity 
can be baked directly into the array cells

35
00:02:39,280 --> 00:02:44,840
during inference for additional efficiency.
Since the output of the array-cells are bounded,

36
00:02:44,840 --> 00:02:49,160
the baked-in values can be easily 
quantized for more efficiency.

37
00:02:49,160 --> 00:02:54,680
Both these optimizations are 
extensively used in our applications.

38
00:02:54,680 --> 00:02:58,160
And finally, we interpolate the 
adjacent cells to obtain,

39
00:02:58,160 --> 00:03:03,800
the pointer value "y". Although we use linear 
interpolants, we note that any higher order

40
00:03:03,800 --> 00:03:11,320
interpolant may be suitable in this context as 
long as the interpolant is differentiable.

41
00:03:11,320 --> 00:03:15,240
Next, we use the pointer "y" 
to query the cascaded array.

42
00:03:15,240 --> 00:03:23,920
We again fetch bounding cells and interpolate 
the values to obtain the final output.

43
00:03:23,920 --> 00:03:28,040
For the backward pass, we compare 
the output with the target

44
00:03:28,040 --> 00:03:33,080
and compute a loss which we then use for 
backpropagation through the arrays.

45
00:03:33,080 --> 00:03:36,480
We compute the gradient of the 
loss w.r.t the output and

46
00:03:36,480 --> 00:03:39,080
at the first node, we multiply 
the incoming gradient with

47
00:03:39,080 --> 00:03:46,480
another gradient w.r.t. array cell content. 
This updates the cells in the cascaded array.

48
00:03:46,480 --> 00:03:50,040
Note that, computing this gradient is 
not unique to our technique; in fact,

49
00:03:50,040 --> 00:03:54,920
several prior work rely on this gradient 
to learn feature grid representations.

50
00:03:54,920 --> 00:03:57,520
However, our technique uses 
a second set of gradient

51
00:03:57,520 --> 00:04:02,400
w.r.t the input coordinate or the 
pointer value "y". This gradient is

52
00:04:02,400 --> 00:04:06,800
used for backpropagation and for 
updating the primary array.

53
00:04:06,800 --> 00:04:11,440
At the second node, following a similar recipe, 
we multiply the incoming gradient with another

54
00:04:11,440 --> 00:04:15,560
gradient w.r.t the array cells.
Finally, the other gradient w.r.t.

55
00:04:16,080 --> 00:04:20,840
input coordinates can be used for 
multi-level indirection, if desired.

56
00:04:20,840 --> 00:04:25,480
With this we conclude the technical 
summary of differentiable indirection.

57
00:04:25,480 --> 00:04:29,600
Next, we look at an intuitive explanation 
of differentiable indirection.

58
00:04:30,960 --> 00:04:34,880
We illustrate this in a 1D setting 
using a simple 1D array.

59
00:04:34,880 --> 00:04:38,760
We begin by representing the 
array content as a histogram.

60
00:04:38,760 --> 00:04:44,600
To obtain a continuous output, we interpolate 
the adjacent cells in the array as shown.

61
00:04:44,600 --> 00:04:50,040
Next, we visualize the two gradients we 
compute at each node during backpropagation

62
00:04:50,040 --> 00:04:52,880
The first gradient w.r.t 
to array cell contents,

63
00:04:52,880 --> 00:04:57,360
and the second w.r.t. to 
array input coordinates.

64
00:04:57,360 --> 00:05:02,080
The purpose of the first gradient is 
to update the array values while

65
00:05:02,080 --> 00:05:06,680
the second gradient is used to learn 
pointer values in the primary array.

66
00:05:06,680 --> 00:05:10,600
On our histogram, we represent the 
gradient w.r.t. to the cell values

67
00:05:10,600 --> 00:05:13,800
using the vertical blue arrows 
as shown on left, while

68
00:05:13,800 --> 00:05:19,200
the gradients w.r.t. to the input coordinate 
is shown in orange with horizontal arrows.

69
00:05:19,200 --> 00:05:24,320
Let us take a moment to 
analyze the two gradients.

70
00:05:24,320 --> 00:05:28,440
The gradient w.r.t to the cell 
content, indicated by blue arrows,

71
00:05:28,440 --> 00:05:34,400
adjusts the cell heights in the histogram.
But the gradient with respect to input coordinate

72
00:05:34,400 --> 00:05:41,800
y is trying to adjust the cell widths.
Let's look more closely at the second gradient.

73
00:05:41,800 --> 00:05:48,840
On this slide, we highlight the intention to 
move the cell boundaries using orange arrows.

74
00:05:48,840 --> 00:05:54,720
We also represent a hypothetical array 
in red with variable cell boundaries.

75
00:05:54,720 --> 00:05:58,440
In such case, the gradients represented 
by the orange arrows should move the cell

76
00:05:58,440 --> 00:06:05,040
boundaries on the red array as shown.
In practice, the information regarding the

77
00:06:05,040 --> 00:06:10,200
cell boundaries is stored as pointers in the 
primary array. The primary and the cascaded

78
00:06:10,200 --> 00:06:16,440
array combined, represents the adaptive resolution 
as illustrated by the hypothetical red array.

79
00:06:16,440 --> 00:06:19,720
With this we conclude our explanation 
regarding adaptive resolution in

80
00:06:19,720 --> 00:06:24,680
differentiable indirection.
Next, we look at a few exemplary

81
00:06:24,680 --> 00:06:29,720
uses of differentiable indirection.
Our technique is useful for compact SDF

82
00:06:29,720 --> 00:06:35,920
representation where we use a simple network 
composed of a 3D primary and 3D cascaded array.

83
00:06:35,920 --> 00:06:39,760
We can also use it for texture compression 
where the network is composed of a 2D

84
00:06:39,760 --> 00:06:43,840
primary and 4D cascaded array. 
We noticed from our experiments

85
00:06:43,840 --> 00:06:49,840
that a higher dimensional cascaded array 
yields better quality compression.

86
00:06:49,840 --> 00:06:52,800
With a small modification to our 
compression only network, we can

87
00:06:52,800 --> 00:06:58,400
jointly treat both compression and filtering.
This technique is useful for real-time rendering

88
00:06:58,400 --> 00:07:04,200
where we no longer need to have separate 
hardware for both compression and filtering.

89
00:07:04,200 --> 00:07:09,720
In the context of shading, we can use our 
technique to represent the Isotropic GGX function,

90
00:07:10,440 --> 00:07:16,520
used for modelling glossy surfaces.
Similarly, we can also model a complex

91
00:07:16,520 --> 00:07:21,480
BRDF such as Disney using our technique 
while being more efficient to evaluate

92
00:07:21,480 --> 00:07:28,240
compared to the reference implementation.
And finally, we apply our technique to radiance

93
00:07:28,240 --> 00:07:34,000
field compression where we compress regular 
grids storing density and view dependent RGB

94
00:07:34,000 --> 00:07:40,360
using two sets of 3D indirections.
Next, we discuss empirical evidence of

95
00:07:40,360 --> 00:07:44,440
efficiency, based on an example.
We illustrate the efficiency of our

96
00:07:44,440 --> 00:07:49,880
technique by approximating isotropic 
GGX, which is a 2D function with a highly

97
00:07:49,880 --> 00:07:54,920
non-linear scalar output.
Since it is a 2D function,

98
00:07:54,920 --> 00:08:01,040
we visualize the function as monochromatic 
texture containing HDR values.

99
00:08:01,040 --> 00:08:05,640
Such function is often used for shading 
glossy objects as shown on the right.

100
00:08:05,640 --> 00:08:09,200
In this context, we compare and 
approximate Isotropic GGX using a

101
00:08:09,200 --> 00:08:13,160
variety of neural techniques such as
multi-layered perceptrons,

102
00:08:13,160 --> 00:08:19,160
a single level interpolated lookup,
A combination of single level lookup and MLP

103
00:08:19,160 --> 00:08:24,520
and finally differentiable indirection.
For this task differentiable indirection

104
00:08:24,520 --> 00:08:31,680
requires 1 Kilobyte of parameter space, 36 FLOPs 
of compute and 16 bytes of memory transfer.

105
00:08:31,680 --> 00:08:36,600
Compared to ours, an MLP only network requires 
4 times more parameter space and 2 orders of

106
00:08:36,600 --> 00:08:42,080
magnitude more compute and bytes transfer.
Whereas a single level grid requires fewer

107
00:08:42,080 --> 00:08:46,120
compute and byte transfer, the benefits 
are outweighed by the enormous parameter

108
00:08:46,120 --> 00:08:52,440
space required by a single level lookup.
Finally, a combination of MLP and lookup strikes

109
00:08:52,440 --> 00:08:58,840
a better balance across the three criteria, but 
overall, our technique still performs better.

110
00:08:58,840 --> 00:09:03,760
Next, we illustrate the efficiency of our 
technique using a few more examples.

111
00:09:03,760 --> 00:09:10,000
For SDFs, we represent 12 GB worth of 
surface sample points in just 24 MB.

112
00:09:10,000 --> 00:09:12,720
Then for texture compression, 
our scheme can compress several

113
00:09:12,720 --> 00:09:18,080
times the raw size of an rgb texure.
For real-time applications, we may also

114
00:09:18,080 --> 00:09:23,160
use it for joint texture compression and 
filtering using a single network.

115
00:09:23,160 --> 00:09:28,200
In the context of shading, we can efficiently 
approximate Disney BRDF by replacing 200+

116
00:09:28,200 --> 00:09:33,520
FLOPs of compute with few lookups.
Finally, differentiable indirection

117
00:09:33,520 --> 00:09:40,280
can compress pre-trained radiance 
fields by orders of magnitude.

118
00:09:40,280 --> 00:09:55,320
The video collage show differentiable indirection 
in action for a variety of applications.

119
00:09:55,320 --> 00:09:58,600
Thus we end our presentation 
with the following conclusion:

120
00:09:58,600 --> 00:10:00,320
that is,
ifferentiable indirection

121
00:10:00,320 --> 00:10:04,920
is an adaptive, highly efficient and flexible 
neural primitive with various applications

122
00:10:04,920 --> 00:10:10,640
in the graphics and potentially beyond.
Thank you for listening to our presentation

123
00:10:10,640 --> 00:10:16,480
and please check our website for more 
details and videos.

